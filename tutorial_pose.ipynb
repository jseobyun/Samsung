{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_pose.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MDN2L2mMCyZG",
        "Kd87IKFvDIrM"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPobZXAKlqHatdhaCMmqNKX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jseobyun/Samsung/blob/master/tutorial_pose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgAL7PFYCDN9",
        "colab_type": "text"
      },
      "source": [
        "# **Dataset connection (from Google drive)**\n",
        "1. Run this cell and click the link\n",
        "\n",
        "2. Copy and paste the authorization code\n",
        "\n",
        "3. Press enter\n",
        "\n",
        "4. If you success, you can see \"Mounted at /content/gdrive/\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT6h_czFRWig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "45f0ef00-3ef9-402f-ada7-47f3f65cbee1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDN2L2mMCyZG",
        "colab_type": "text"
      },
      "source": [
        "# **Dependency check**\n",
        "\n",
        "1. All these libraries should be imported for the tutorial\n",
        "\n",
        "2. If you see some errors, please ask to TA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r3-8_LwDtsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import copy\n",
        "import torch.backends.cudnn as cudnn\n",
        "import time\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.resnet import BasicBlock, Bottleneck\n",
        "from torchvision.models.resnet import model_urls\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.parallel.data_parallel import DataParallel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd87IKFvDIrM",
        "colab_type": "text"
      },
      "source": [
        "# **Util functions**\n",
        "\n",
        "1. All functions in this cell are helper functions used in following codes\n",
        "\n",
        "2. Do not change or revise them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sREvcQAEHVRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def world2cam(world_coord, R, t):\n",
        "    cam_coord = np.dot(R, world_coord.transpose(1,0)).transpose(1,0) + t.reshape(1,3)\n",
        "    return cam_coord\n",
        "\n",
        "def cam2pixel(cam_coord, f, c):\n",
        "    x = cam_coord[:, 0] / (cam_coord[:, 2] + 1e-8) * f[0] + c[0]\n",
        "    y = cam_coord[:, 1] / (cam_coord[:, 2] + 1e-8) * f[1] + c[1]\n",
        "    z = cam_coord[:, 2]\n",
        "    img_coord = np.concatenate((x[:,None], y[:,None], z[:,None]),1)\n",
        "    return img_coord\n",
        "\n",
        "def process_bbox(bbox, width, height):\n",
        "    # sanitize bboxes\n",
        "    x, y, w, h = bbox\n",
        "    x1 = np.max((0, x))\n",
        "    y1 = np.max((0, y))\n",
        "    x2 = np.min((width - 1, x1 + np.max((0, w - 1))))\n",
        "    y2 = np.min((height - 1, y1 + np.max((0, h - 1))))\n",
        "    if w*h > 0 and x2 >= x1 and y2 >= y1:\n",
        "        bbox = np.array([x1, y1, x2-x1, y2-y1])\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    # aspect ratio preserving bbox\n",
        "    w = bbox[2]\n",
        "    h = bbox[3]\n",
        "    c_x = bbox[0] + w/2.\n",
        "    c_y = bbox[1] + h/2.\n",
        "    aspect_ratio = 1\n",
        "    if w > aspect_ratio * h:\n",
        "        h = w / aspect_ratio\n",
        "    elif w < aspect_ratio * h:\n",
        "        w = h * aspect_ratio\n",
        "    bbox[2] = w*1.25\n",
        "    bbox[3] = h*1.25\n",
        "    bbox[0] = c_x - bbox[2]/2.\n",
        "    bbox[1] = c_y - bbox[3]/2.\n",
        "    return bbox\n",
        "\n",
        "  # helper functions\n",
        "def generate_patch_image(cvimg, bbox, scale, rot):\n",
        "    img = cvimg.copy()\n",
        "    img_height, img_width, img_channels = img.shape\n",
        "        \n",
        "    bb_c_x = float(bbox[0] + 0.5*bbox[2])\n",
        "    bb_c_y = float(bbox[1] + 0.5*bbox[3])\n",
        "    bb_width = float(bbox[2])\n",
        "    bb_height = float(bbox[3])\n",
        "    \n",
        "    trans = gen_trans_from_patch_cv(bb_c_x, bb_c_y, bb_width, bb_height, 256, 256, scale, rot, inv=False)\n",
        "    img_patch = cv2.warpAffine(img, trans, (256, 256), flags=cv2.INTER_LINEAR)\n",
        "\n",
        "    img_patch = img_patch[:,:,::-1].copy()\n",
        "    img_patch = img_patch.astype(np.float32)\n",
        "\n",
        "    return img_patch, trans\n",
        "\n",
        "def rotate_2d(pt_2d, rot_rad):\n",
        "    x = pt_2d[0]\n",
        "    y = pt_2d[1]\n",
        "    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n",
        "    xx = x * cs - y * sn\n",
        "    yy = x * sn + y * cs\n",
        "    return np.array([xx, yy], dtype=np.float32)\n",
        "\n",
        "def gen_trans_from_patch_cv(c_x, c_y, src_width, src_height, dst_width, dst_height, scale, rot, inv=False):\n",
        "    # augment size with scale\n",
        "    src_w = src_width * scale\n",
        "    src_h = src_height * scale\n",
        "    src_center = np.array([c_x, c_y], dtype=np.float32)\n",
        "\n",
        "    # augment rotation\n",
        "    rot_rad = np.pi * rot / 180\n",
        "    src_downdir = rotate_2d(np.array([0, src_h * 0.5], dtype=np.float32), rot_rad)\n",
        "    src_rightdir = rotate_2d(np.array([src_w * 0.5, 0], dtype=np.float32), rot_rad)\n",
        "\n",
        "    dst_w = dst_width\n",
        "    dst_h = dst_height\n",
        "    dst_center = np.array([dst_w * 0.5, dst_h * 0.5], dtype=np.float32)\n",
        "    dst_downdir = np.array([0, dst_h * 0.5], dtype=np.float32)\n",
        "    dst_rightdir = np.array([dst_w * 0.5, 0], dtype=np.float32)\n",
        "\n",
        "    src = np.zeros((3, 2), dtype=np.float32)\n",
        "    src[0, :] = src_center\n",
        "    src[1, :] = src_center + src_downdir\n",
        "    src[2, :] = src_center + src_rightdir\n",
        "\n",
        "    dst = np.zeros((3, 2), dtype=np.float32)\n",
        "    dst[0, :] = dst_center\n",
        "    dst[1, :] = dst_center + dst_downdir\n",
        "    dst[2, :] = dst_center + dst_rightdir\n",
        "\n",
        "    if inv:\n",
        "        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n",
        "    else:\n",
        "        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n",
        "\n",
        "    return trans\n",
        "\n",
        "def trans_point2d(pt_2d, trans):\n",
        "    src_pt = np.array([pt_2d[0], pt_2d[1], 1.]).T\n",
        "    dst_pt = np.dot(trans, src_pt)\n",
        "    return dst_pt[0:2]\n",
        "\n",
        "def soft_argmax(heatmaps, joint_num=18):\n",
        "\n",
        "    heatmaps = heatmaps.reshape((-1, joint_num, 64*64*64))\n",
        "    heatmaps = F.softmax(heatmaps, 2)\n",
        "    heatmaps = heatmaps.reshape((-1, joint_num, 64, 64, 64))\n",
        "\n",
        "    accu_x = heatmaps.sum(dim=(2,3))\n",
        "    accu_y = heatmaps.sum(dim=(2,4))\n",
        "    accu_z = heatmaps.sum(dim=(3,4))\n",
        "\n",
        "    accu_x = accu_x * torch.cuda.comm.broadcast(torch.arange(1,64+1).type(torch.cuda.FloatTensor), devices=[accu_x.device.index])[0]\n",
        "    accu_y = accu_y * torch.cuda.comm.broadcast(torch.arange(1,64+1).type(torch.cuda.FloatTensor), devices=[accu_y.device.index])[0]\n",
        "    accu_z = accu_z * torch.cuda.comm.broadcast(torch.arange(1,64+1).type(torch.cuda.FloatTensor), devices=[accu_z.device.index])[0]\n",
        "\n",
        "    accu_x = accu_x.sum(dim=2, keepdim=True) -1\n",
        "    accu_y = accu_y.sum(dim=2, keepdim=True) -1\n",
        "    accu_z = accu_z.sum(dim=2, keepdim=True) -1\n",
        "\n",
        "    coord_out = torch.cat((accu_x, accu_y, accu_z), dim=2)\n",
        "\n",
        "    return coord_out\n",
        "\n",
        "\n",
        "def vis_keypoints(img, kps, kp_thresh=0.4, alpha=1):\n",
        "    kps_lines = ( (0, 7), (7, 8), (8, 9), (9, 10), (8, 11), (11, 12), (12, 13), (8, 14), (14, 15), (15, 16), (0, 1), (1, 2), (2, 3), (0, 4), (4, 5), (5, 6) )\n",
        "    # Convert from plt 0-1 RGBA colors to 0-255 BGR colors for opencv.\n",
        "    cmap = plt.get_cmap('rainbow')\n",
        "    colors = [cmap(i) for i in np.linspace(0, 1, len(kps_lines) + 2)]\n",
        "    colors = [(c[2] * 255, c[1] * 255, c[0] * 255) for c in colors]\n",
        "\n",
        "    # Perform the drawing on a copy of the image, to allow for blending.\n",
        "    kp_mask = np.copy(img)\n",
        "\n",
        "    # Draw the keypoints.\n",
        "\n",
        "    for l in range(len(kps_lines)):\n",
        "        i1 = kps_lines[l][0]\n",
        "        i2 = kps_lines[l][1]\n",
        "\n",
        "        p1 = kps[0, i1].astype(np.int32), kps[1, i1].astype(np.int32)\n",
        "        p2 = kps[0, i2].astype(np.int32), kps[1, i2].astype(np.int32)\n",
        "\n",
        "        if kps[2, i1] > kp_thresh and kps[2, i2] > kp_thresh:\n",
        "            cv2.line(\n",
        "                kp_mask, p1, p2,\n",
        "                color=colors[l], thickness=2, lineType=cv2.LINE_AA)\n",
        "        if kps[2, i1] > kp_thresh:\n",
        "            cv2.circle(\n",
        "                kp_mask, p1,\n",
        "                radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n",
        "        if kps[2, i2] > kp_thresh:\n",
        "            cv2.circle(\n",
        "                kp_mask, p2,\n",
        "                radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n",
        "\n",
        "    # Blend the keypoints.\n",
        "    return cv2.addWeighted(img, 1.0 - alpha, kp_mask, alpha, 0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "823xkkxvDZ4z",
        "colab_type": "text"
      },
      "source": [
        "# **Data preprocessing & Data loading**\n",
        "\n",
        "1. Human3.6M dataset consists of images(1000x1000x3)  and 3D body joints\n",
        "\n",
        "2. I crop the images using the bounding box and convert them to (256x256x3) maintaining the aspect ratio\n",
        "\n",
        "3. I convert 3D body joitns to 2D body joints by camera projection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptQrjNdvIu-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_loc = '/content/gdrive/My Drive'\n",
        "class Human36M:\n",
        "    def __init__(self, data_split):\n",
        "        self.data_split = data_split\n",
        "        self.img_dir = os.path.join(data_loc, 'HUMAN36M', 'images')\n",
        "        self.annot_path = os.path.join(data_loc, 'HUMAN36M', 'annotations')                \n",
        "        self.joints_name = ('Pelvis', 'R_Hip', 'R_Knee', 'R_Ankle', 'L_Hip', 'L_Knee', 'L_Ankle', 'Torso', 'Neck', 'Nose', 'Head', 'L_Shoulder', 'L_Elbow', 'L_Wrist', 'R_Shoulder', 'R_Elbow', 'R_Wrist', 'Thorax')\n",
        "        self.action_name = ['Directions', 'Discussion', 'Eating', 'Greeting', 'Phoning', 'Posing', 'Purchases', 'Sitting', 'SittingDown', 'Smoking', 'Photo', 'Waiting', 'Walking', 'WalkDog', 'WalkTogether']\n",
        "        self.root_idx = self.joints_name.index('Pelvis')\n",
        "        self.lshoulder_idx = self.joints_name.index('L_Shoulder')\n",
        "        self.rshoulder_idx = self.joints_name.index('R_Shoulder')        \n",
        "        self.data = self.load_data()\n",
        "\n",
        "    def get_subsampling_ratio(self):\n",
        "        if self.data_split == 'train':\n",
        "            return 5\n",
        "        elif self.data_split == 'test':\n",
        "            return 64\n",
        "        else:\n",
        "            assert 0, print('Unknown subset')\n",
        "\n",
        "    def get_subject(self):\n",
        "        if self.data_split == 'train':\n",
        "            subject = [11] # train set and test set should not be same but we use same data only for the tutorial\n",
        "        elif self.data_split == 'test':\n",
        "            subject = [11]          \n",
        "        return subject\n",
        "    \n",
        "    def add_thorax(self, joint_coord):\n",
        "        thorax = (joint_coord[self.lshoulder_idx, :] + joint_coord[self.rshoulder_idx, :]) * 0.5\n",
        "        thorax = thorax.reshape((1, 3))\n",
        "        joint_coord = np.concatenate((joint_coord, thorax), axis=0)\n",
        "        return joint_coord\n",
        "\n",
        "    def load_data(self):\n",
        "        print('Load data of H36M')\n",
        "        subject_list = self.get_subject()\n",
        "        sampling_ratio = self.get_subsampling_ratio()\n",
        "        \n",
        "        # aggregate annotations from each subject\n",
        "        db = COCO()\n",
        "        cameras = {}\n",
        "        joints = {}\n",
        "        for subject in subject_list:\n",
        "            # data load\n",
        "            with open(os.path.join(self.annot_path, 'Human36M_subject' + str(subject) + '_data_new.json'),'r') as f:\n",
        "                annot = json.load(f)\n",
        "            if len(db.dataset) == 0:\n",
        "                for k,v in annot.items():\n",
        "                    db.dataset[k] = v\n",
        "            else:\n",
        "                for k,v in annot.items():\n",
        "                    db.dataset[k] += v\n",
        "            # camera load\n",
        "            with open(os.path.join(self.annot_path, 'Human36M_subject' + str(subject) + '_camera.json'),'r') as f:\n",
        "                cameras[str(subject)] = json.load(f)\n",
        "            # joint coordinate load\n",
        "            with open(os.path.join(self.annot_path, 'Human36M_subject' + str(subject) + '_joint_3d.json'),'r') as f:\n",
        "                joints[str(subject)] = json.load(f)\n",
        "        db.createIndex()       \n",
        "        \n",
        "        data = []\n",
        "        for aid in db.anns.keys():\n",
        "            ann = db.anns[aid]\n",
        "            image_id = ann['image_id']\n",
        "            img = db.loadImgs(image_id)[0]\n",
        "            img_path = os.path.join(self.img_dir, img['file_name'])\n",
        "            img_width, img_height = img['width'], img['height']\n",
        "           \n",
        "            # check subject and frame_idx\n",
        "            subject = img['subject']; frame_idx = img['frame_idx'];\n",
        "            if subject not in subject_list:\n",
        "                continue\n",
        "            if frame_idx % sampling_ratio != 0:\n",
        "                continue\n",
        "\n",
        "            # camera parameter\n",
        "            cam_idx = img['cam_idx']\n",
        "            cam_param = cameras[str(subject)][str(cam_idx)]\n",
        "            R,t,f,c = np.array(cam_param['R'], dtype=np.float32), np.array(cam_param['t'], dtype=np.float32), np.array(cam_param['f'], dtype=np.float32), np.array(cam_param['c'], dtype=np.float32)\n",
        "                \n",
        "            # project world coordinate to cam, image coordinate space\n",
        "            action_idx = img['action_idx']; subaction_idx = img['subaction_idx']; frame_idx = img['frame_idx'];\n",
        "            joint_world = np.array(joints[str(subject)][str(action_idx)][str(subaction_idx)][str(frame_idx)], dtype=np.float32)\n",
        "            joint_world = self.add_thorax(joint_world)\n",
        "            joint_cam = world2cam(joint_world, R, t)\n",
        "            joint_img = cam2pixel(joint_cam, f, c)\n",
        "            joint_img[:,2] = joint_img[:,2] - joint_cam[self.root_idx,2]            \n",
        "            \n",
        "            bbox = process_bbox(np.array(ann['bbox']), img_width, img_height)\n",
        "            if bbox is None: continue            \n",
        "               \n",
        "            data.append({\n",
        "                'img_path': img_path,\n",
        "                'img_id': image_id,\n",
        "                'bbox': bbox,\n",
        "                'joint_img': joint_img}) # [org_img_x, org_img_y, depth - root_depth]             \n",
        "           \n",
        "        return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZSQ8xiYdoDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DatasetLoader(Dataset):\n",
        "    def __init__(self, db, is_train, transform):\n",
        "        \n",
        "        self.db = db.data           \n",
        "        self.transform = transform\n",
        "        self.is_train = is_train        \n",
        "\n",
        "    def __getitem__(self, index):       \n",
        "        data = copy.deepcopy(self.db[index])\n",
        "\n",
        "        bbox = data['bbox']\n",
        "        joint_img = data['joint_img']        \n",
        "\n",
        "        # 1. load image\n",
        "        cvimg = cv2.imread(data['img_path'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
        "        if not isinstance(cvimg, np.ndarray):\n",
        "            raise IOError(\"Fail to read %s\" % data['img_path'])\n",
        "        img_height, img_width, img_channels = cvimg.shape\n",
        "             \n",
        "        # 2. crop patch from img and perform data augmentation (flip, rot, color scale, synthetic occlusion)\n",
        "        scale, rot= 1.0, 0.0\n",
        "        img_patch, trans = generate_patch_image(cvimg, bbox, scale, rot)        \n",
        "\n",
        "        # 3. generate patch joint ground truth\n",
        "        # apply Affine Transform on joints       \n",
        "        for i in range(len(joint_img)):\n",
        "            joint_img[i, 0:2] = trans_point2d(joint_img[i, 0:2], trans)\n",
        "            joint_img[i, 2] /= 1000. \n",
        "            joint_img[i, 2] = (joint_img[i,2] + 1.0)/2. # 0~1 normalize            \n",
        "\n",
        "        # change coordinates to output space\n",
        "        joint_img[:, 0] = joint_img[:, 0] / 4\n",
        "        joint_img[:, 1] = joint_img[:, 1] / 4\n",
        "        joint_img[:, 2] = joint_img[:, 2] * 64\n",
        "        \n",
        "        if self.is_train:\n",
        "            img_patch = self.transform(img_patch)           \n",
        "            joint_img = joint_img.astype(np.float32)         \n",
        "            return img_patch, joint_img\n",
        "        else:\n",
        "            img_patch = self.transform(img_patch)                        \n",
        "            return img_patch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.db)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7FP8jUtFM4p",
        "colab_type": "text"
      },
      "source": [
        "# **Train dataset prepration**\n",
        "\n",
        "1. By using above data preparation codes, train dataset is generated.\n",
        "\n",
        "2. In this tutorial, we will use batch size 16\n",
        "\n",
        "3. If you success up to here, you can see \\\n",
        "\"Load data of H36M\"\\\n",
        " \"creating index...\"\\\n",
        "  \"index created!\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjdGa1R8fExB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "86edb195-c46b-4382-d666-6b65394999a2"
      },
      "source": [
        "trainset = Human36M(\"train\")\n",
        "trainset_loader = DatasetLoader(trainset, True, transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225))]))\n",
        "batch_generator = DataLoader(dataset=trainset_loader, batch_size=16, shuffle=True, num_workers=20, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data of H36M\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwY5R3o6F8jC",
        "colab_type": "text"
      },
      "source": [
        "# **Model construction**\n",
        "\n",
        "1. Our network consists of an encoder(ResNet) and an decoder(your network)\n",
        "\n",
        "2. In this part, you will design the decoder using deconvolution layers\n",
        "\n",
        "3. Output of the encoder is (2048,8,8) feature and your decoder should decode this feature to (64*18,64,64)\n",
        "\n",
        "  *(18 is the number of body joints)*\n",
        "\n",
        "4. First, focus on how to change the width and the height of the feature from (8,8) to (64, 64)\n",
        "\n",
        "5. Second, add the final layer(1x1 convolution) to change the number of the channels to (64*18)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSAL5AYemy-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "84dfe3f1-2bf2-4fba-cb9d-612411889dc6"
      },
      "source": [
        "# Encoder\n",
        "class ResNetBackbone(nn.Module):\n",
        "\n",
        "    def __init__(self, resnet_type):\n",
        "\t\n",
        "        resnet_spec = {18: (BasicBlock, [2, 2, 2, 2], [64, 64, 128, 256, 512], 'resnet18'),\n",
        "\t\t       34: (BasicBlock, [3, 4, 6, 3], [64, 64, 128, 256, 512], 'resnet34'),\n",
        "\t\t       50: (Bottleneck, [3, 4, 6, 3], [64, 256, 512, 1024, 2048], 'resnet50'),\n",
        "\t\t       101: (Bottleneck, [3, 4, 23, 3], [64, 256, 512, 1024, 2048], 'resnet101'),\n",
        "\t\t       152: (Bottleneck, [3, 8, 36, 3], [64, 256, 512, 1024, 2048], 'resnet152')}\n",
        "        block, layers, channels, name = resnet_spec[resnet_type]\n",
        "        \n",
        "        self.name = name\n",
        "        self.inplanes = 64\n",
        "        super(ResNetBackbone, self).__init__()\n",
        "\n",
        "        # Early stage before ResBlocks\n",
        "        # Conv1 : input channel 3, output channel 64, kernel size 7, stride 2, padding 3, bias = False\n",
        "        # bn1 : input channel 64\n",
        "        # relu : inplace = True\n",
        "        # maxpool : kernel size 3, stride 2, padding 1\n",
        "\n",
        "        # Do not change the structure.\n",
        "        # If you change the struccture, pre-trained ResNet50 cannot be loaded\n",
        "\n",
        "        #------------Your code here------------#\n",
        "        self.conv1 = \n",
        "        self.bn1 = \n",
        "        self.relu =  # inplace=True\n",
        "        self.maxpool = \n",
        "        # -------------------------------------#\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        #initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):                \n",
        "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "    # make ResBlocks\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Early stage before ResBlocks\n",
        "        # conv1-bn1-relu-maxpool\n",
        "        # ------------Your code here------------#\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        # --------------------------------------#\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def init_weights(self):\n",
        "        org_resnet = torch.utils.model_zoo.load_url(model_urls[self.name])\n",
        "        # drop orginal resnet fc layer, add 'None' in case of no fc layer, that will raise error\n",
        "        org_resnet.pop('fc.weight', None)\n",
        "        org_resnet.pop('fc.bias', None)\n",
        "        self.load_state_dict(org_resnet)\n",
        "        print(\"Initialize resnet from model zoo\")\n",
        "\n",
        "# Decoder\n",
        "class HeadNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.inplanes = 2048\n",
        "        self.outplanes = 256\n",
        "\n",
        "        super(HeadNet, self).__init__()\n",
        "\n",
        "        # Design the deconv layers\n",
        "        # nn.BatchNor2d() and nn.ReLU(inplace=True) should follow each deconv layer\n",
        "        # ------------Your code here------------#\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        # --------------------------------------#\n",
        "\n",
        "        \n",
        "        # Design the final 1x1 conv layer\n",
        "        # It is for changing the channel of the feature to 18*64\n",
        "        # ------------Your code here------------#\n",
        "        self.final_layer = \n",
        "        # --------------------------------------#\n",
        "   \n",
        "    def forward(self, x):\n",
        "        # ------------Your code here------------#\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        # --------------------------------------#        \n",
        "        x = self.final_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def init_weights(self):\n",
        "        # After you design your own deconv layers\n",
        "        # Uncomment this initialization and change the names      \n",
        "        # Reapeat fro all deconv layers\n",
        "\n",
        "        # for name, m in self.deconv1.named_modules(): #self.deconv1 -> self.your deconv layer name\n",
        "        #     if isinstance(m, nn.ConvTranspose2d):\n",
        "        #         nn.init.normal_(m.weight, std=0.001)\n",
        "        #     elif isinstance(m, nn.BatchNorm2d):\n",
        "        #         nn.init.constant_(m.weight, 1)\n",
        "        #         nn.init.constant_(m.bias, 0)        \n",
        "\n",
        "        for m in self.final_layer.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, std=0.001)\n",
        "                nn.init.constant_(m.bias, 0)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-682c827c4646>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    self.conv1 =\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DC3b3xhnFKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResPoseNet(nn.Module):\n",
        "    def __init__(self, backbone, head):\n",
        "        super(ResPoseNet, self).__init__()\n",
        "        self.backbone = backbone\n",
        "        self.head = head\n",
        "        \n",
        "\n",
        "    def forward(self, input_img, target=None):\n",
        "        fm = self.backbone(input_img) # encoder\n",
        "        hm = self.head(fm) # decoder, hm denotes the heatmap\n",
        "\n",
        "        # ------------Your code here------------#\n",
        "        coord =     # heatmap to joint coordinates, apply soft_argmax(heatmap)              \n",
        "        # --------------------------------------#       \n",
        "        \n",
        "        return coord        \n",
        "\n",
        "# get overall framework consists of the encoder and the decoder\n",
        "def get_pose_net(is_train):\n",
        "    \n",
        "    backbone = ResNetBackbone(50)\n",
        "    head_net = HeadNet()\n",
        "    if is_train:\n",
        "        backbone.init_weights()\n",
        "        head_net.init_weights()\n",
        "\n",
        "    model = ResPoseNet(backbone, head_net)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKWMYZFgCfwo",
        "colab_type": "text"
      },
      "source": [
        "# **Optimizer and loss function design**\n",
        "\n",
        "Choose your down optimizer [e.g. Adam, SGD] and loss function [e.g. nn.L1Loss(), nn.L2Loss()]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0elUVvIjnNA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = get_pose_net(is_train=True)\n",
        "model = DataParallel(model).cuda()\n",
        "# ------------Your code here------------#\n",
        "optimizer = \n",
        "criterion =         \n",
        "# --------------------------------------#  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpCPw33qCyuM",
        "colab_type": "text"
      },
      "source": [
        "# **Train the network**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2LO8snvoWFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cudnn.fastest = True\n",
        "cudnn.benchmark = True\n",
        "for epoch in range(4):\n",
        "  for itr, (input_img, joint_img) in enumerate(batch_generator):     \n",
        "      target_coord = joint_img.cuda()      \n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      pred_coord = model(input_img)\n",
        "\n",
        "      # get the loss and do backward and step\n",
        "      # ------------Your code here------------#\n",
        "\n",
        "      \n",
        "      \n",
        "      # --------------------------------------#             \n",
        "      print(f\"[Epoch {epoch} / Iter {itr} ] loss : {loss.item()}\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6HwsqNUF-az",
        "colab_type": "text"
      },
      "source": [
        "# **Test the network**\n",
        "\n",
        "In this tutorial, we evaluate only qualitative results of the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NY6vG-DAmOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cudnn.fastest = True\n",
        "cudnn.benchmark = True\n",
        "cudnn.deterministic = False\n",
        "cudnn.enabled = True\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for itr, (input_img, _) in enumerate(batch_generator):            \n",
        "        # forward\n",
        "        pred_coord = model(input_img)\n",
        "\n",
        "        #visualize\n",
        "        filename = str(itr)\n",
        "        tmpimg = input_img[0].cpu().numpy()\n",
        "        tmpimg = tmpimg * np.array([0.229, 0.224, 0.225]).reshape(3,1,1) + np.array([0.485, 0.456, 0.406]).reshape(3,1,1)\n",
        "        tmpimg = tmpimg.astype(np.uint8)        \n",
        "        tmpimg = np.transpose(tmpimg,(1,2,0)).copy()\n",
        "        tmpkps = np.zeros((3,18))\n",
        "        tmpkps[:2,:] = pred_coord[0,:,:2].cpu().numpy().transpose(1,0) *4\n",
        "        tmpkps[2,:] = 1\n",
        "        tmpimg = vis_keypoints(tmpimg, tmpkps)\n",
        "        plt.imshow(tmpimg)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        if itr > 10:\n",
        "          break\n",
        "plt.close()\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}